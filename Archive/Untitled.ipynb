{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53bc856-0e4e-49af-a08b-b8d861948b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba0e728-ff79-4465-9692-97aa28886e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_'\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts( \\\n",
    "        make_request( \\\n",
    "            URI_TEMPLATE.format( \\\n",
    "                subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request( \\\n",
    "                URI_TEMPLATE.format( \\\n",
    "                    subreddit, new_start_at, end_at, SIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "    return post_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f05d27a-2cc1-487a-8c36-7f6525e7f756",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_request' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m end_at \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(datetime\u001b[38;5;241m.\u001b[39mutcnow()\u001b[38;5;241m.\u001b[39mtimestamp())\n\u001b[1;32m      3\u001b[0m start_at \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor((datetime\u001b[38;5;241m.\u001b[39mutcnow() \u001b[38;5;241m-\u001b[39m               \n\u001b[1;32m      4\u001b[0m                        timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m365\u001b[39m))\u001b[38;5;241m.\u001b[39mtimestamp())\n\u001b[0;32m----> 5\u001b[0m posts \u001b[38;5;241m=\u001b[39m \u001b[43mpull_posts_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubreddit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_at\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_at\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [3], line 14\u001b[0m, in \u001b[0;36mpull_posts_for\u001b[0;34m(subreddit, start_at, end_at)\u001b[0m\n\u001b[1;32m     10\u001b[0m SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     11\u001b[0m URI_TEMPLATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.pushshift.io/reddit/search/submission?subreddit=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m&after=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m&before=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m&size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m post_collections \u001b[38;5;241m=\u001b[39m map_posts( \\\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mmake_request\u001b[49m( \\\n\u001b[1;32m     15\u001b[0m         URI_TEMPLATE\u001b[38;5;241m.\u001b[39mformat( \\\n\u001b[1;32m     16\u001b[0m             subreddit, start_at, end_at, SIZE))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(post_collections)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m==\u001b[39m SIZE:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_request' is not defined"
     ]
    }
   ],
   "source": [
    "subreddit = 'Coffee'\n",
    "end_at = math.ceil(datetime.utcnow().timestamp())\n",
    "start_at = math.floor((datetime.utcnow() - \\              \n",
    "                       timedelta(days=365)).timestamp())\n",
    "posts = pull_posts_for(subreddit, start_at, end_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70b44e-5b83-4b12-bc16-4b4ec5f3ba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70609f1a-268c-4328-a965-b0c4a7077a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditSpider(scrapy.Spider):\n",
    "    name = \"reddit\"\n",
    "    allowed_domains = [\"reddit.com\"]\n",
    "    start_urls = [\n",
    "        \"https://www.reddit.com/r/Coffee\"\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response): \n",
    "        hxs = Selector(response) \n",
    "        for item in hxs.xpath(\"//p[@class='_1qeIAgB0cPwnLhDF9XSiJM']/text()\").extract():\n",
    "            items.append(item)\n",
    "        return items\n",
    "\n",
    "pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29731c39-bc16-4447-a747-83ea909c538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwanted\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_reddit(subreddit=None, listing=None, limit=None, timeframe=None):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        base_url = f'https://www.reddit.com/r/{subreddit}/{listing}.json?limit={limit}&t={timeframe}'\n",
    "        \n",
    "        request = requests.get(base_url, headers = {'User-agent': 'yourbot'},\n",
    "                              verify=False)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print('An Error Occured')\n",
    "        \n",
    "    return request.json()\n",
    "\n",
    "r = get_reddit(subreddit='tea', listing='hot', limit=1000, timeframe='all')\n",
    "\n",
    "def get_post_titles(r):\n",
    "\n",
    "    posts = []\n",
    "    \n",
    "    for post in r['data']['children']:\n",
    "        x = post['data']['title']\n",
    "        posts.append(x)\n",
    "        \n",
    "    return posts\n",
    "\n",
    "posts = get_post_titles(r)\n",
    "\n",
    "def get_results(r):\n",
    "\n",
    "    myDict = {}\n",
    "    \n",
    "    for post in r['data']['children']:\n",
    "        \n",
    "        myDict[post['data']['title']] = {'url':post['data']['url'],\n",
    "                                         'post_url':'https://www.reddit.com'+post['data']['permalink'],\n",
    "                                         'score':post['data']['score'],\n",
    "                                         'comments':post['data']['num_comments']}\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(myDict, orient='index')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = get_results(r)\n",
    "\n",
    "def get_reddit_post(url):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        url = url + '.json'\n",
    "        \n",
    "        request = requests.get(url, headers = {'User-agent': 'yourbot'},\n",
    "                              verify=False)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print('An Error Occured')\n",
    "        \n",
    "    content = request.json()\n",
    "    \n",
    "    for d in content:\n",
    "        \n",
    "        for dictionary in d['data']['children']:\n",
    "            \n",
    "            c = dictionary['data']['selftext']\n",
    "            \n",
    "            break\n",
    "            \n",
    "        break\n",
    "        \n",
    "    return c\n",
    "\n",
    "df['post_content'] = df.post_url.map(get_reddit_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b7a08-b5aa-4d2d-9331-0fa29419d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run the tests again by adding coffee and tea into stopwords \n",
    "\n",
    "# include the common words found on the threads\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_stopwords2 = english_stopwords.extend(['hi', 'all', 'everyone', 'thank you', 'hello', 'mod', 'edit', 'etc', 'im', 'me', 'ive', 'thank', 'coffee', 'tea'])\n",
    "\n",
    "def filtered_text_2(words):\n",
    "    \"\"\"Remove common english stop words from an array, and those which are numbers\"\"\"\n",
    "    filtered_words_2 = [word for word in words if (word not in english_stopwords2) and (not word.isdigit())]\n",
    "    return filtered_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41beb155-30b5-41e6-a1e4-7dc36e43e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter away the coffee and tea \n",
    "merged_df['filtered_tokens_2'] = merged_df['tokens'].apply(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8835c5c-c771-4a57-b5d4-245524cd419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then lemmatize it\n",
    "merged_df['lem_tokens_2'] = merged_df['filtered_tokens_2'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae1f49-ba68-47f8-b800-94f3886b0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = merged_df[\"lem_tokens_2\"].map(' '.join)\n",
    "X = text\n",
    "y = merged_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849dc95-5d1a-4d88-8e35-3adf35251bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914cbe4b-019c-4eb3-ae69-f746309450c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec2 = CountVectorizer(min_df=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ab444-a9e5-40a2-b085-3ddfdd93490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7d538-7c45-4515-9de5-0c6e4d0763d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(cvec2.fit_transform(X_train).todense(), \n",
    "                          columns=cvec2.get_feature_names())\n",
    "\n",
    "# plot top occuring words\n",
    "X_train_df.sum().sort_values(ascending=False).head(40).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ba3d8-f83f-4312-bc85-dd24aaec42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91130fdb-076c-4142-9611-1a7c22837e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c303224-efbf-46c9-bf77-1c654d4f1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = GridSearchCV(pipe2, \n",
    "                  param_grid=pipe_params, \n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d8621-7a0a-4f2e-8ad6-c366616a70c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92829a0-4170-4b26-beff-a0b1faf255be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda77a9f-0d9f-4d00-bdff-b5a95e375815",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4126b-859b-4102-b739-2b7455586c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds = gs2.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c583d-0f63-4c94-863d-308c47cac7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View confusion matrix\n",
    "plot_confusion_matrix(gs, X_test, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c4f4a-1841-41c0-977d-d7be59f38858",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = tn / (tn + fp)\n",
    "print('Specificity for count vectorizer with Naive Bayes:', spec)\n",
    "\n",
    "# this performs worse than the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c55ad-96eb-4f89-b948-6ba2bbe9fd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac261d-2613-48a5-82df-9497e4af34b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef1b51-8cef-4552-ba15-3f1e660a2e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da04e7-fc00-410d-9227-447d9f4936b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi-sg",
   "language": "python",
   "name": "dsi-sg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
